(* Conditional Generative Adversarial Nets trained on the MNIST dataset. *)

let image_dim = MnistHelper.image_dim in
let label_count = MnistHelper.label_count in
let latent_dim = 100 in
let generator_hidden_nodes = 128 in
let discriminator_hidden_nodes = 128 in
let batch_size = 256 in
let learning_rate = 0.0001 in
let batches = 100000000 in

(* The generator receives as input both some noise and a one-hot encoding of labels. *)
let gen_create_generator {shape : List Nat} =
  &(fun(vs : VarStore) ->
      let linear1 =
        ~(Layer.gen_linear {shape}
            #input_dim (latent_dim + label_count)
            #output_dim generator_hidden_nodes
          )
          vs
          Layer.Activation.leaky_relu
      in
      let linear2 =
        ~(Layer.gen_linear {shape}
            #input_dim generator_hidden_nodes
            #output_dim image_dim
          )
          vs
          Layer.Activation.tanh
      in
      fun (rand_input : Tensor %shape) -> ~Layer.gen_forward linear1 rand_input |> ~Layer.gen_forward linear2
   )
in

(* The discriminator receives as input both an image and a one-hot encoding of labels. *)
let gen_create_discriminator {shape : List Nat} =
  &(fun(vs : VarStore) ->
      let linear1 =
        ~(Layer.gen_linear {shape}
            #input_dim (image_dim + label_count)
            #output_dim discriminator_hidden_nodes
          )
          vs
          Layer.Activation.leaky_relu
      in
      let linear2 =
        ~(Layer.gen_linear {shape}
            #input_dim discriminator_hidden_nodes
            #output_dim 1
          )
          vs
          Layer.Activation.sigmoid
      in
      fun (xs : Tensor %shape) -> ~Layer.gen_forward linear1 xs |> ~Layer.gen_forward linear2
  )
in

&(let bce #labels (labels : Float) (model_values : Tensor %[]) =
    let epsilon = 0.0000001 in
    let open Tensor in
      ~gen_sub
        ~(gen_zeros ([] as List Nat))
        ( ~gen_add
            ( ~gen_mult
                (f labels)
                (~gen_log (~gen_add model_values (f epsilon)))
            )
            ( ~gen_mult
              (f (1.0 -. labels))
              (~gen_log (~gen_sub (f (1.0 +. epsilon)) model_values))
            )
        )
    |> ~Tensor.gen_mean
  in
  let rand (u : Unit) =
    let open Tensor in
    ~gen_sub
      (~gen_mult (f 2.0) ~(gen_rand [batch_size, latent_dim]))
      (f 1.0)
  in
  let write_samples (samples : Z) #filename (filename : String) =
    IO.OutChannel.with_file filename (fun (channel : OutChannel) ->
      IO.OutChannel.output_string channel "data_ = [\n";
      (range 0 99) |> List.iter (fun(sample_index : Int) ->
        List.init image_dim (fun (pixel_index : Int) ->
          Tensor.get_float2 samples sample_index pixel_index |> Printf.sprintf "%.2f"
        )
          |> String.concat #sep ", "
          |> Printf.sprintf "  [%s],\n"
          |> IO.OutChannel.output_string channel
      );
      IO.OutChannel.output_string channel "]\n"
    )
  in
  let mnist = MnistHelper.read_files () in
  let generator_vs = VarStore.create #name "gen" () in
  let generator = create_generator generator_vs in
  let opt_g = Optimizer.adam generator_vs #learning_rate learning_rate in
  let discriminator_vs = VarStore.create #name "disc" () in
  let discriminator = create_discriminator discriminator_vs in
  let opt_d = Optimizer.adam discriminator_vs #learning_rate learning_rate in
  let fixed_noise = rand () in
  let fake_labels =
    (* Generate some regular one-hot encoded labels. *)
    List.init ~(lift_int (batch_size * label_count)) (fun (i : Int) ->
      if mod i label_count == mod (i / label_count) label_count then 1.0 else 0.0
    )
      |> Tensor.float_vec
      |> ~(Tensor.gen_reshape #shape [batch_size, label_count])
  in
  let generator (xs : A) =
    (* EDIT: Use 2-ary cat *)
    let ys = Tensor.cat_ xs fake_labels #dim 1 |> generator in
    Tensor.cat_ ys fake_labels #dim 1
  in
  let one = ~(Tensor.gen_ones []) in
  (range 1 batches) |> List.iter (fun (batch_idx : Int) ->
    let (batch_images, batch_labels) =
      DatasetHelper.train_batch mnist #batch_size batch_size #batch_idx batch_idx
    in
    let onehot_labels =
      ~(Tensor.gen_zeros [batch_size, label_count])
        |> Tensor.scatter_ #dim 1 #src one #index batch_labels
    in
    let real_input =
      (* EDIT: Use 2-ary cat *)
      let open Tensor in
      (cat_ ((f 2.0 * batch_images) - f 1.0) onehot_labels) #dim 1
    in
    let discriminator_loss =
      ~Tensor.gen_add
        (bce #labels 0.9 (discriminator real_input))
        (bce #labels 0.0 (rand () |> generator |> discriminator))
    in
    Optimizer.backward_step #loss discriminator_loss opt_d;
    let generator_loss = bce #labels 1.0 (rand () |> generator |> discriminator) in
    Optimizer.backward_step #loss generator_loss opt_g;
    if mod batch_idx 100 == 0 then
      Stdio.printf
        "batch %4d    d-loss: %12.6f    g-loss: %12.6f\n%!"
        batch_idx
        (Tensor.float_value discriminator_loss)
        (Tensor.float_value generator_loss)
    else
      ();
    Gc.full_major ();
    if mod batch_idx 25000 == 0 || (batch_idx < 100000 && mod batch_idx 5000 == 0)
    then
      write_samples
        (generator fixed_noise)
        #filename (Printf.sprintf "out%d.txt" batch_idx)
    else
      ()
  )
)
