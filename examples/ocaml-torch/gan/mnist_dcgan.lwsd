(* Deep Convolutional Generative Adversarial Networks trained on the MNIST dataset.
   https://arxiv.org/abs/1511.06434
*)

let image_w = MnistHelper.image_width in
let image_h = MnistHelper.image_height in
let image_dim = MnistHelper.image_dim in
let latent_dim = 100 in

(* use the standard GAN or RaLSGAN (i.e., Relativistic average LSGAN;
   see https://ajolicoeur.wordpress.com/RelativisticGAN/) *)
let use_sandard_gan = true in
let batch_size = 128 in

let gen_create_generator {shape : List Nat} =
  &(fun(vs : VarStore) ->
      let convt1 =
        ~(Layer.gen_conv_transpose2d_
            #ksize 7 #stride 1 #padding 0 #output_padding 0
            #input_dim 100
            #output_dim 64
            {shape}
          )
          vs
          Layer.Activation.none
      in
      let convt2 =
        ~(Layer.gen_conv_transpose2d_
            #ksize 4 #stride 2 #padding 1 #output_padding 0
            #input_dim 64
            #output_dim 32
            {[8894, 7763, 6651, 5548]} (* TODO *)
          )
          vs
          Layer.Activation.none
      in
      let convt3 =
        ~(Layer.gen_conv_transpose2d_
            #ksize 4 #stride 2 #padding 1 #output_padding 0
            #input_dim 32
            #output_dim 1
            {[763, 652, 541, 437]} (* TODO *)
          )
          vs
          Layer.Activation.tanh
      in
      fun (rand_input : Tensor %shape) ->
        rand_input
          |> ~Layer.gen_forward convt1
          |> ~Tensor.gen_const_batch_norm
          |> ~Tensor.gen_relu
          |> ~Layer.gen_forward convt2
          |> ~Tensor.gen_const_batch_norm
          |> ~Tensor.gen_relu
          |> ~Layer.gen_forward convt3
  )
in

let gen_create_discriminator {shape : List Nat} =
  &(fun(vs : VarStore) ->
      let conv1 =
        ~(Layer.gen_conv2d_ {List.nth 0 shape} {List.nth 2 shape} {List.nth 3 shape}
            #ksize 4 #stride 2 #padding 1
            #input_dim 1
            #output_dim 32
          )
          vs
          Layer.Activation.none
      in
      let conv2 =
        ~(Layer.gen_conv2d_ {207} {308} {409} (* TODO *)
            #ksize 4 #stride 2 #padding 1
            #input_dim 32
            #output_dim 64
          )
          vs
          Layer.Activation.none
      in
      let conv3 =
        ~(Layer.gen_conv2d_ {501} {602} {703} (* TODO *)
            #ksize 7 #stride 1 #padding 0
            #input_dim 64
            #output_dim 1
          )
          vs
          Layer.Activation.sigmoid
      in
      fun (xs : Tensor %shape) ->
        ~Layer.gen_forward conv1 xs
          |> ~Tensor.gen_leaky_relu
          |> ~Layer.gen_forward conv2
          |> ~Tensor.gen_const_batch_norm
          |> ~Tensor.gen_leaky_relu
          |> ~Layer.gen_forward conv3
  )
in

let gen_bce {shape : List Nat} =
  &(fun #labels (labels : Float) ->
    fun(model_values : Tensor %shape) ->
      let epsilon = 0.0000001 in
      let open Tensor in
      ~gen_sub
        ~(gen_zeros ([] as List Nat))
        ( ~gen_add
            ( ~gen_mult
                (f labels)
                (~gen_log (~gen_add model_values (f epsilon)))
            )
            ( ~gen_mult
              (f (1.0 -. labels))
              (~gen_log (~gen_sub (f (1.0 +. epsilon)) model_values))
            )
        )
        |> ~Tensor.gen_mean
      )
in

&(let device = Device.cpu in
  let learning_rate = 0.0001 in
  let batches = 100000000 in
  let random (u : Unit) =
    let open Tensor in
    ~gen_sub
      (~gen_mult (f 2.0) ~(gen_rand [batch_size, latent_dim, 1, 1]))
      (f 1.0)
  in
  let end_line_by (s : String) = "  [" ++ s ++ "],\n" in
  let write_samples (samples : Tensor %[4423]) #filename (filename : String) = (* TODO *)
    IO.OutChannel.with_file filename (fun (channel : OutChannel) ->
      IO.OutChannel.output_string channel "data_ = [\n";
      (range 0 99) |> List.iter (fun(sample_index : Int) ->
        List.initialize ~(lift_int image_dim) (fun (pixel_index : Int) ->
          ~Tensor.gen_get_float2_unsafe samples sample_index pixel_index |> show_float
        )
          |> String.concat #sep ", "
          |> end_line_by
          |> IO.OutChannel.output_string channel
      );
      IO.OutChannel.output_string channel "]\n"
    )
  in
  let mnist = MnistHelper.dataset in
  let generator_vs = VarStore.create #frozen false #name "gen" #device device () in
  let opt_g = Optimizer.adam generator_vs #learning_rate learning_rate in
  let discriminator_vs = VarStore.create #frozen false #name "disc" #device device () in
  let discriminator = ~gen_create_discriminator discriminator_vs in
  let opt_d = Optimizer.adam discriminator_vs #learning_rate learning_rate in
  let fixed_noise = rand () in
  (range 1 batches) |> List.iter (fun(batch_idx : Int) ->
    let (batch_images, dummy) =
      ~Dataset_helper.gen_train_batch
        mnist #batch_size batch_size #batch_idx batch_idx
    in
    let batch_images =
      ~(Tensor.gen_reshape #shape [batch_size, 1, image_w, image_h]) batch_images
    in
    let discriminator_loss =
      ~(if use_standard_gan then
          &(~Tensor.gen_add
              (~gen_bce #labels 0.9 (discriminator (let open Tensor in f 2.0 * batch_images - f 1.0)))
              (~gen_bce #labels 0.0 (random () |> ~gen_create_generator generator_vs |> discriminator))
          )
        else
          &(let y_pred = discriminator (let open Tensor in (f 2.0 * batch_images) - f 1.0) in
            let y_pred_fake = random () |> ~gen_create_generator generator_vs |> discriminator in
            let open Tensor in
            ~gen_add
              (y_pred - mean y_pred_fake - f 1.0 |> square |> mean)
              (y_pred_fake - mean y_pred + f 1.0 |> square |> mean)
          )
      )
    in
    Optimizer.backward_step opt_d #loss discriminator_loss;
    let generator_loss =
      ~(if use_standard_gan then
          bce #labels 1.0 (random () |> ~gen_create_generator generator_vs |> discriminator)
        else
          let y_pred = discriminator (let open Tensor in f 2.0 * batch_images - f 1.0) in
          let y_pred_fake = random () |> ~gen_create_generator generator_vs |> discriminator in
          let open Tensor in
          ~gen_add
            (y_pred - mean y_pred_fake + f 1.0 |> square |> mean)
            (y_pred_fake - mean y_pred - f 1.0 |> square |> mean)
      )
    in
    Optimizer.backward_step opt_g #loss generator_loss;
    if mod batch_idx 100 == 0 then
      Stdio.printf
        "batch %4d    d-loss: %12.6f    g-loss: %12.6f\n%!"
        batch_idx
        (Tensor.float_value discriminator_loss)
        (Tensor.float_value generator_loss)
    else
      ();
    Gc.full_major ();
    if mod batch_idx 25000 == 0 || (batch_idx < 100000 && mod batch_idx 5000 == 0) then
      write_samples
        (generator fixed_noise |> Tensor.reshape #shape [ -1; image_dim ])
        #filename (Printf.sprintf "out%d.txt" batch_idx)
    else
      ()
  )
)
