(* Conditional Generative Adversarial Nets trained on the MNIST dataset. *)

let image_dim = MnistHelper.image_dim in
let label_count = MnistHelper.label_count in
let latent_dim = 100 in
let generator_hidden_nodes = 128 in
let discriminator_hidden_nodes = 128 in
let batch_size = 256 in

(* The generator receives as input both some noise and a one-hot encoding of labels. *)
let gen_create_generator {shape : {s : List Nat | List.length s > 0}} =
  &(fun(vs : VarStore) ->
      let linear1 =
        ~(Layer.gen_linear {List.init shape}
            #input_dim (latent_dim + label_count)
            #output_dim generator_hidden_nodes
          )
          vs
          #use_bias true
          Layer.Activation.leaky_relu
      in
      let linear2 =
        ~(Layer.gen_linear {List.init shape}
            #input_dim generator_hidden_nodes
            #output_dim image_dim
          )
          vs
          #use_bias true
          Layer.Activation.tanh
      in
      fun (rand_input : Tensor %shape) ->
        ~Layer.gen_forward linear1 rand_input |> ~Layer.gen_forward linear2
   )
in

(* The discriminator receives as input both an image and a one-hot encoding of labels. *)
let gen_create_discriminator
    {shape : {s : List Nat | List.length s > 0 && List.last s == image_dim + label_count}} =
  &(fun(vs : VarStore) ->
      let linear1 =
        ~(Layer.gen_linear {List.init shape}
            #input_dim (image_dim + label_count)
            #output_dim discriminator_hidden_nodes
          )
          vs
          #use_bias true
          Layer.Activation.leaky_relu
      in
      let linear2 =
        ~(Layer.gen_linear {List.init shape}
            #input_dim discriminator_hidden_nodes
            #output_dim 1
          )
          vs
          #use_bias true
          Layer.Activation.sigmoid
      in
      fun (xs : Tensor %shape) ->
        ~Layer.gen_forward linear1 xs |> ~Layer.gen_forward linear2
  )
in

let gen_bce {shape : List Nat} =
  &(fun #labels (labels : Float) ->
    fun(model_values : Tensor %shape) ->
      let epsilon = 0.0000001 in
      let open Tensor in
      ( ~gen_sub
          (~gen_mult (f (1.0 -. labels)) (~gen_log (~gen_sub (f (1.0 +. epsilon)) model_values)))
          (~gen_mult (f labels) (~gen_log (~gen_add model_values (f epsilon))))
      )
        |> ~Tensor.gen_mean
  )
in

&(let device = Device.cpu in
  let learning_rate = 0.0001 in
  let batches = 100000000 in

  let random (u : Unit) =
    let open Tensor in
    ~gen_sub (~gen_mult (f 2.0) ~(gen_rand [batch_size, latent_dim])) (f 1.0)
  in
  let end_line_by (s : String) = "  [" ++ s ++ "],\n" in
  let write_samples (samples : Tensor %[batch_size, image_dim + label_count]) #filename (filename : String) =
    IO.OutChannel.with_file filename (fun (channel : OutChannel) ->
      IO.OutChannel.output_string channel "data_ = [\n";
      (range 0 99) |> List.iter (fun(sample_index : Int) ->
        List.initialize ~(lift_int image_dim) (fun (pixel_index : Int) ->
          ~Tensor.gen_get_float2_unsafe samples sample_index pixel_index |> show_float
        )
          |> String.concat #sep ", "
          |> end_line_by
          |> IO.OutChannel.output_string channel
      );
      IO.OutChannel.output_string channel "]\n"
    )
  in
  let mnist = MnistHelper.dataset in
  let generator_vs = VarStore.create #frozen false #name "gen" #device device () in
  let opt_g = Optimizer.adam generator_vs #learning_rate learning_rate in
  let discriminator_vs = VarStore.create #frozen false #name "disc" #device device () in
  let opt_d = Optimizer.adam discriminator_vs #learning_rate learning_rate in
  let fixed_noise = random () in
  let fake_labels =
    (* Generate some regular one-hot encoded labels. *)
    ~(List.initialize (batch_size * label_count) (fun (i : Int) ->
        if mod i label_count == mod (i // label_count) label_count then 1.0 else 0.0
      )
        |> Tensor.gen_float_vec
    )
      |> ~(Tensor.gen_reshape #shape [batch_size, label_count])
  in
  let generator (xs : Tensor %[batch_size, latent_dim]) =
    let ys = ~(Tensor.gen_cat_ #dim 1) xs fake_labels |> ~gen_create_generator generator_vs in
    ~(Tensor.gen_cat_ #dim 1) ys fake_labels
  in
  let one = ~(Tensor.gen_ones ([] as List Nat)) in
  (range 1 batches) |> List.iter (fun (batch_idx : Int) ->
    let (batch_images, batch_labels) =
      ~(DatasetHelper.gen_train_batch #batch_size batch_size)
        device
        mnist
        batch_idx
    in
    let onehot_labels =
      ~(Tensor.gen_zeros [batch_size, label_count])
        |> ~(Tensor.gen_scatter_ #dim 1) #src one #index batch_labels
    in
    let real_input =
      let open Tensor in
      ~(gen_cat_ #dim 1)
        (~gen_sub (~gen_mult (f 2.0) batch_images) (f 1.0))
        onehot_labels
    in
    let discriminator_loss =
      ~Tensor.gen_add
        (~gen_bce #labels 0.9 (~gen_create_discriminator discriminator_vs real_input))
        (~gen_bce #labels 0.0 (random () |> generator |> ~gen_create_discriminator discriminator_vs))
    in
    Optimizer.backward_step #clip_grad Optimizer.ClipGrad.none opt_d #loss discriminator_loss;
    let generator_loss =
      ~gen_bce #labels 1.0 (random () |> generator |> ~gen_create_discriminator discriminator_vs)
    in
    Optimizer.backward_step #clip_grad Optimizer.ClipGrad.none opt_g #loss generator_loss;
    if mod batch_idx 100 == 0 then
      ( print_string "batch";
        print_int batch_idx;
        print_string "d-loss:";
        print_float (Tensor.float_value discriminator_loss);
        print_string "g-loss";
        print_float (Tensor.float_value generator_loss)
      )
    else
      ();
    Gc.full_major ();
    if mod batch_idx 25000 == 0 || (batch_idx < 100000 && mod batch_idx 5000 == 0) then
      write_samples
        (generator fixed_noise)
        #filename ("out" ++ show_int batch_idx ++ ".txt")
    else
      ()
  )
)
