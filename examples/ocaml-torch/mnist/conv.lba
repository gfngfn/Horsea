let batch_size = 256 in

&(let epochs = 5000 in
  let learning_rate = 0.001 in
  let device = ~(Device.gen_cuda_if_available ()) in
  let vs = VarStore.create #frozen false #name "cnn" #device device () in
  ~(let gen_conv2d1 {size : Nat} =
      &(fun (t : Tensor %[size, 1, 28, 28]) ->
          ~(Layer.gen_conv2d_
              #ksize 5
              #stride 1
              #padding 0
              #input_dim 1
              #output_dim 32
            )
            vs
            Layer.Activation.none
            t
      )
    in
    let gen_conv2d2 {size : Nat} =
      &(fun(t : Tensor %[size, 32, 12, 12]) ->
          ~(Layer.gen_conv2d_
              #ksize 5
              #stride 1
              #padding 0
              #input_dim 32
              #output_dim 64
            )
            vs
            Layer.Activation.none
            t
      )
    in
    let gen_linear1 {size : Nat} =
      &(~(Layer.gen_linear
            {[size]}
            #input_dim 1024
            #output_dim 1024
          )
          vs
          Layer.Activation.relu
      )
    in
    let gen_linear2 {size : Nat} =
      &(~(Layer.gen_linear
            {[size]}
            #input_dim 1024
            #output_dim MnistHelper.label_count
          )
          vs
          Layer.Activation.none
      )
    in
    &(let adam = Optimizer.adam vs #learning_rate learning_rate in
      ~(let gen_model (size : Nat) =
          &(fun #is_training (is_training : Bool) ->
            fun(xs : Tensor %[size, MnistHelper.image_dim]) ->
              ~(Tensor.gen_reshape #shape [size, 1, 28, 28]) xs
                |> ~Layer.gen_forward ~(gen_conv2d1 {size})
                |> ~(Tensor.gen_max_pool2d #padding (0, 0) #ksize (2, 2) #stride (2, 2))
                |> ~Layer.gen_forward ~(gen_conv2d2 {size})
                |> ~(Tensor.gen_max_pool2d #padding (0, 0) #ksize (2, 2) #stride (2, 2))
                |> ~(Tensor.gen_reshape #shape [size, 1024])
                |> ~Layer.gen_forward ~(gen_linear1 {size})
                |> ~Tensor.gen_dropout #p 0.5 #is_training is_training
                |> ~Layer.gen_forward ~(gen_linear2 {size})
          )
        in
        let gen_train_model (size : Nat) = &(~(gen_model size) #is_training true) in
        let gen_test_model (size : Nat) = &(~(gen_model size) #is_training false) in
        &((range 1 epochs) |> List.iter (fun(batch_idx : Int) ->
            let (batch_images, batch_labels) =
              ~(DatasetHelper.gen_train_batch #batch_size batch_size)
                device
                MnistHelper.dataset
                batch_idx
            in
            let loss =
              ~(Tensor.gen_cross_entropy_for_logits {batch_size} {MnistHelper.label_count})
                (~(gen_train_model batch_size) batch_images)
                #targets batch_labels
            in
            Optimizer.backward_step adam #loss loss;
            if mod batch_idx 50 == 0 then
              let test_accuracy =
                ~(DatasetHelper.gen_batch_accuracy
                    _ _ _ _ {MnistHelper.label_count}
                    #batch_size batch_size
                    #predict gen_test_model
                  )
                  device
                  MnistHelper.dataset
              in
              print_int batch_idx;
              print_float (Tensor.float_value loss);
              print_float test_accuracy
            else
              ()
          )
        )
      )
    )
  )
)