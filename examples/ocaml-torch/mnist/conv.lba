let batch_size = 256 in

&(let epochs = 5000 in
  let learning_rate = 0.001 in
  let device = ~(Device.gen_cuda_if_available ()) in
  let vs = VarStore.create #frozen false #name "cnn" #device device () in
  let conv2d1 (t : Tensor %[batch_size, 1, 28, 28]) =
    ~(Layer.gen_conv2d_ #ksize 5 #stride 1 #padding 0 #input_dim 1 #output_dim 32)
      vs
      Layer.Activation.none
      t
  in
  let conv2d2 (t : Tensor %[batch_size, 32, 12, 12]) =
    ~(Layer.gen_conv2d_ #ksize 5 #stride 1 #padding 0 #input_dim 32 #output_dim 64)
      vs
      Layer.Activation.none
      t
  in
  let linear1 =
    ~(Layer.gen_linear #input_dim 1024 #output_dim 1024 {[batch_size, 1024]})
      vs
      #use_bias true
      Layer.Activation.relu
  in
  let linear2 =
    ~(Layer.gen_linear #input_dim 1024 #output_dim MnistHelper.label_count {[batch_size, 1024]})
      vs
      #use_bias true
      Layer.Activation.none
  in
  let adam = Optimizer.adam vs #learning_rate learning_rate in
  let model #is_training (is_training : Bool) (xs : Tensor %[batch_size, MnistHelper.image_dim]) =
    ~(Tensor.gen_reshape #shape [batch_size, 1, 28, 28]) xs
      |> ~Layer.gen_forward conv2d1
      |> ~(Tensor.gen_max_pool2d #padding (0, 0) #ksize (2, 2) #stride (2, 2))
      |> ~Layer.gen_forward conv2d2
      |> ~(Tensor.gen_max_pool2d #padding (0, 0) #ksize (2, 2) #stride (2, 2))
      |> ~(Tensor.gen_reshape #shape [batch_size, 1024])
      |> ~Layer.gen_forward linear1
      |> ~Tensor.gen_dropout #p 0.5 #is_training is_training
      |> ~Layer.gen_forward linear2
  in
  let train_model = model #is_training true in
  let test_model = model #is_training false in
  (range 1 epochs) |> List.iter (fun(batch_idx : Int) ->
    let (batch_images, batch_labels) =
      ~(DatasetHelper.gen_train_batch #batch_size batch_size)
        device
        MnistHelper.dataset
        batch_idx
    in
    let loss =
      ~(Tensor.gen_cross_entropy_for_logits {batch_size} {MnistHelper.label_count})
        (train_model batch_images)
        #targets batch_labels
    in
    Optimizer.backward_step #clip_grad Optimizer.ClipGrad.none adam #loss loss;
    if mod batch_idx 50 == 0 then
      let test_accuracy =
        ~(DatasetHelper.gen_batch_accuracy {MnistHelper.label_count} #batch_size batch_size)
          #predict test_model
          device
          MnistHelper.dataset
      in
      print_int batch_idx;
      print_float (~Tensor.gen_float_value loss);
      print_float test_accuracy
    else
      ()
  )
)
