(* This example uses the tinyshakespeare dataset which can be downloaded at:
   https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

   It has been heavily inspired by https://github.com/karpathy/char-rnn
*)

let learning_rate = 0.01 in
let hidden_size = 256 in
let seq_len = 180 in
let batch_size = 256 in
let epochs = 100 in
let sampling_length = 1024 in

let gen_sample {input_size : Nat} {hidden_size : Nat} =
  &(fun #dataset (dataset : TextHelper) ->
    fun #lstm (lstm : Lstm %input_size %hidden_size) ->
    fun #linear (linear : Tensor %[1, hidden_size] -> Tensor %[624]) -> (* TODO *)
    fun #device (device : Device) ->
      let labels = TextHelper.labels dataset in
      let (seq, dummy_state) =
        let zero_state = ~(Layer.Lstm.gen_zero_state #batch_size 1) lstm in
        let rec loop (non_empty_seq : Int * List Int) (state : Tensor %[1, hidden_size] * Tensor %[1, hidden_size]) (i : Int) : List Int * (Tensor %[1, hidden_size] * Tensor %[1, hidden_size]) =
          let (prev_label, seq_tail) = non_empty_seq in
          let seq = prev_label :: seq_tail in
          if i == 0 then
            (seq, state)
          else
            ( (* Gc.full_major (); *)
              let prev_y = ~(Tensor.gen_zeros [1, labels]) (* #device device *) in
              Tensor.fill_float (Tensor.narrow prev_y #dim 1 #start prev_label #length 1) 1.0;
              let state = ~Layer.Lstm.gen_step lstm state prev_y in
              let (h, dummy) = state in
              let sampled_y =
                ~Layer.gen_forward linear h
                  |> Tensor.softmax #dim -1
                  |> Tensor.multinomial #num_samples 1 #replacement false
              in
              let sampled_label =
                Tensor.get (Tensor.get sampled_y 0) 0 |> Tensor.int_value
              in
              loop (sampled_label, seq) state (i - 1))
        in
        loop (0, []) zero_state sampling_length
      in
      List.rev_map seq (fun (label : Int) -> TextHelper.char dataset #label label)
        |> String.of_char_list
  )
in

&(let device = Device.cuda_if_available () in
  let dataset = TextHelper.create #filename "data/input.txt" in
  let vs = Var_store.create #name "char-rnn" #device device () in
  let labels = TextHelper.labels dataset in
  Stdio.printf
    "Dataset loaded, length: %d, labels: %d"
    (TextHelper.total_length dataset)
    labels;
  let lstm = ~(Layer.Lstm.gen_create #input_dim labels #hidden_size hidden_size) vs in
  let linear = Layer.linear vs #input_dim hidden_size labels in
  let adam = Optimizer.adam vs #learning_rate learning_rate in
  let batches_per_epoch = (TextHelper.total_length dataset - seq_len) / batch_size in
  Checkpointing.loop
    #start_index 1
    #end_index epochs
    #var_stores [vs]
    #checkpoint_base "char-rnn.ot"
    #checkpoint_every (iters 1)
    (fun #index (epoch_idx : Int) ->
      Stdio.Out_channel.write_all
        (Printf.sprintf "out.txt.%d" epoch_idx)
        #data (~gen_sample #lstm lstm #linear linear #dataset dataset #device device);
      let start_time = Unix.gettimeofday () in
      let sum_loss = Tensor.f 0.0 in
      TextHelper.iter #device device dataset #batch_size batch_size #seq_len seq_len
        (fun (batch_idx : Int) -> fun #xs (xs : D) -> fun #ys (ys : E) ->
          Optimizer.zero_grad adam;
          let onehot =
            let xs = Tensor.view xs #size [ batch_size; seq_len; 1 ] in
            let one = Tensor.ones (Tensor.size xs) #device device in
            Tensor.zeros [batch_size, seq_len, labels] #device device
            |> Tensor.scatter_ #dim 2 #src one #index xs
          in
          let (lstm_out, dummy) = ~Layer.Lstm.gen_seq lstm onehot #is_training true in
          let logits = Layer.forward linear lstm_out in
          (* Compute the cross-entropy loss. *)
          let loss =
            Tensor.cross_entropy_for_logits
              (Tensor.view logits #size [batch_size * seq_len, labels])
              #targets (Tensor.view ys #size [batch_size * seq_len])
          in
          (let open Tensor in sum_loss += loss);
          Stdio.printf
            "%d/%d %f"
            batch_idx
            batches_per_epoch
            (Tensor.float_value sum_loss / float (1 + batch_idx));
          Tensor.backward loss;
          Optimizer.step #clip_grad (norm2 4.0) adam
        );
      Stdio.printf
        "%d %.0fs %f"
        epoch_idx
        (Unix.gettimeofday () - start_time)
        (Tensor.float_value sum_loss / float batches_per_epoch)
    )
)
