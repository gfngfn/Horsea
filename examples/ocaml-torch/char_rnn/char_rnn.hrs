(* This example uses the tinyshakespeare dataset which can be downloaded at:
   https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

   It has been heavily inspired by https://github.com/karpathy/char-rnn
*)

let hidden_size = 256 in
let seq_len = 180 in
let batch_size = 256 in
let labels = 32 in (* TODO *)

let sample {labels : Nat} {input_size : Nat} {hidden_size : Nat}
    #dataset (dataset : TextHelper labels)
    #lstm (lstm : Lstm input_size hidden_size)
    #linear (linear : Tensor [1, hidden_size] -> Tensor [1, labels])
    #device (device : Device) =
  let (seq, dummy_state) =
    let zero_state = Layer.Lstm.zero_state #batch_size 1 lstm in
    let rec loop
        (non_empty_seq : Int * List Int)
        (state : Tensor [1, hidden_size] * Tensor [1, hidden_size])
        (i : Int) : List Int * (Tensor [1, hidden_size] * Tensor [1, hidden_size]) =
      let (prev_label, seq_tail) = non_empty_seq in
      let seq = prev_label :: seq_tail in
      if i == 0 then
        (seq, state)
      else
        ( Gc.full_major ();
          let prev_y = Tensor.zeros [1, labels] (* #device device *) in
          Tensor.fill_float (Tensor.narrow #dim 1 #length 1 #start prev_label prev_y) 1.0;
          let state = Layer.Lstm.step lstm state prev_y in
          let (h, dummy) = state in
          let sampled_y =
            Layer.forward linear h
              |> Tensor.softmax #dim -1
              |> Tensor.multinomial {[1]} {labels} #num_samples 1 #replacement false
          in
          let sampled_label =
            Tensor.get {1} {[] as List Nat}
              (Tensor.get {1} {[1]} sampled_y 0) 0
                |> Tensor.int_value
          in
          loop (sampled_label, seq) state (i - 1)
        )
    in
    let sampling_length = 1024 in
    loop (0, [] as List Int) zero_state sampling_length
  in
  seq
    |> List.rev_map (fun (label : Int) -> TextHelper.char dataset #label label)
    |> String.from_char_list
in

let learning_rate = 0.01 in
let epochs = 100 in
let device = Device.cuda_if_available () in
let dataset = TextHelper.create {labels} #filename "data/input.txt" in
let vs = VarStore.create #frozen false #name "char-rnn" #device device () in
print_string "length:";
print_int (TextHelper.total_length dataset);
print_string "labels:";
print_int (lift_int labels);
let lstm = Layer.Lstm.create #input_dim labels #hidden_size hidden_size vs in
let linear =
  Layer.linear {[1]} #input_dim hidden_size #output_dim labels vs Layer.Activation.none
in
let adam = Optimizer.adam vs #learning_rate learning_rate in
let batches_per_epoch =
  (TextHelper.total_length dataset - (lift_int seq_len)) // (lift_int batch_size)
in
Checkpointing.loop
  #start_index 1
  #end_index epochs
  #var_stores [vs]
  #checkpoint_base "char-rnn.ot"
  #checkpoint_every_iters 1
  (fun #index (epoch_idx : Int) ->
    IO.write_all
      ("out.txt." ++ show_int epoch_idx)
      #data (sample #dataset dataset #lstm lstm #linear linear #device device);
    let start_time = Unix.gettimeofday () in
    let sum_loss = Tensor.f 0.0 in
    TextHelper.iter #seq_len seq_len #batch_size batch_size
      ( fun (batch_idx : Int) ->
        fun #xs (xs : Tensor [batch_size, seq_len]) ->
        fun #ys (ys : Tensor [batch_size, seq_len]) ->
          Optimizer.zero_grad adam;
          let onehot =
            let xs = Tensor.view #size [batch_size, seq_len, 1] xs in
            let one = Tensor.ones [batch_size, seq_len, 1] (* #device device *) in
            Tensor.zeros [batch_size, seq_len, labels] (* #device device *)
            |> Tensor.scatter_ #dim 2 #src one #index xs
          in
          let (lstm_out, dummy) = Layer.Lstm.seq #is_training true lstm onehot in
          let logits = Layer.forward linear lstm_out in
          (* Compute the cross-entropy loss. *)
          let loss =
            Tensor.cross_entropy_for_logits
              (Tensor.view #size [batch_size * seq_len, labels] logits)
              #targets (Tensor.view #size [batch_size * seq_len] ys)
          in
          (let open Tensor in sum_loss += loss);
          print_int (lift_int batch_idx);
          print_int batches_per_epoch;
          print_float (Tensor.float_value sum_loss / float (1 + (lift_int batch_idx)));
          Tensor.backward loss;
          Optimizer.step #clip_grad (Optimizer.ClipGrad.norm2 4.0) adam
      )
      #device device
      dataset;
    print_int epoch_idx;
    print_int (Unix.gettimeofday () - start_time);
    print_float (Tensor.float_value sum_loss / float batches_per_epoch)
  )
