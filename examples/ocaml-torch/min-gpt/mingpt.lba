(* This example uses the tinyshakespeare dataset which can be downloaded at:
   https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

   It has been heavily inspired by https://github.com/karpathy/minGPT
*)

let batch_size = 64 in
let seq_len = 128 in

let gen_causal_self_attention
    {sz_b : Nat}
    {sz_t : Nat}
    {sz_c : Nat}
    #block_size (cfg_block_size : Nat)
    #n_head (cfg_n_head : Nat)
    #n_embd (cfg_n_embd : {n : Int | mod n cfg_n_head == 0}) =
  &(fun #attn_pdrop (cfg_attn_pdrop : Float) ->
    fun #resid_pdrop (cfg_resid_pdrop : Float) ->
    fun #embd_pdrop (cfg_embd_pdrop : Float) ->
    fun(vs : VarStore) ->
      let linear (n : String) =
        ~(Layer.gen_linear {[sz_b, sz_t]} #input_dim cfg_n_embd #output_dim cfg_n_embd) (* TODO: sz_c == cfg_n_embd ? *)
          (let open VarStore in vs / n)
          #use_bias true
          Layer.Activation.none
      in
      let key = linear "key" in
      let query = linear "query" in
      let value = linear "value" in
      let proj = linear "proj" in
      let mask_init =
        ~(Tensor.gen_ones [cfg_block_size, cfg_block_size]) (* #device (VarStore.device vs) *)
          |> ~Tensor.gen_tril #diagonal 0
          |> ~(Tensor.gen_view #size [1, 1, cfg_block_size, cfg_block_size])
      in
      let mask = ~Tensor.gen_eq_scalar mask_init (Tensor.f 0.0) in
      ~Layer.gen_of_fn_ (fun #is_training (is_training : Bool) -> fun (xs : Tensor %[sz_b, sz_t, sz_c]) ->
        ~(let sz_c_over_n_head = sz_c // cfg_n_head in
          let size = [sz_b, sz_t, cfg_n_head, sz_c_over_n_head] in
          &(let k = (* Tensor %[sz_b, cfg_n_head, sz_t, sz_c / cfg_n_head] *)
              xs
                |> ~Layer.gen_forward key
                |> ~(Tensor.gen_view #size size)
                |> ~(Tensor.gen_transpose #dim0 1 #dim1 2)
            in
            let q =
              xs
                |> ~Layer.gen_forward query
                |> ~(Tensor.gen_view #size size)
                |> ~(Tensor.gen_transpose #dim0 1 #dim1 2)
            in
            let v =
              xs
                |> ~Layer.gen_forward value
                |> ~(Tensor.gen_view #size size)
                |> ~(Tensor.gen_transpose #dim0 1 #dim1 2)
            in
            let att = (* Tensor %[sz_b, cfg_n_head, sz_t, sz_t] *)
              ~Tensor.gen_matmul q (~(Tensor.gen_transpose #dim0 -2 #dim1 -1) k)
                as Tensor %[sz_b, cfg_n_head, sz_t, sz_t]
            in
            let att0 = (~Tensor.gen_div att (Tensor.f (sqrt (float ~(lift_int sz_c_over_n_head))))) in
            (* let a = mask + 1 in *)
            let att =
              att0
                |> ~Tensor.gen_masked_fill #mask mask #value (Tensor.f neg_infinity)
                |> ~(Tensor.gen_softmax #dim -1)
                |> ~Tensor.gen_dropout #p cfg_attn_pdrop #is_training is_training
            in
            ~Tensor.gen_matmul att v (* [sz_b, n_head, sz_t, sz_c / n_head] *)
              |> ~(Tensor.gen_transpose #dim0 1 #dim1 2) (* [sz_b, sz_t, n_head, sz_c / n_head] *)
              |> ~Tensor.gen_contiguous
              |> ~(Tensor.gen_view #size [sz_b, sz_t, sz_c])
              |> ~Layer.gen_forward proj
              |> ~Tensor.gen_dropout #p cfg_resid_pdrop #is_training is_training
          )
        )
      )
  )
in

let gen_block
    {sz_b : Nat} {sz_t : Nat} {sz_c : Nat}
    #block_size (cfg_block_size : Int)
    #n_head (cfg_n_head : Int)
    #n_embd (cfg_n_embd : {n : Int | mod n cfg_n_head == 0}) =
  &(fun #attn_pdrop (cfg_attn_pdrop : Float) ->
    fun #resid_pdrop (cfg_resid_pdrop : Float) ->
    fun #embd_pdrop (cfg_embd_pdrop : Float) ->
    fun(vs : VarStore) ->
      let ln1 = ~(Layer.gen_layer_norm #dim cfg_n_embd {[sz_b, sz_t, sz_c]}) (let open VarStore in vs / "ln1") in
      let ln2 = ~(Layer.gen_layer_norm #dim cfg_n_embd {[sz_b, sz_t, sz_c]}) (let open VarStore in vs / "ln2") in
      let attn =
        ~(gen_causal_self_attention
            {sz_b}
            {sz_t}
            {sz_c}
            #block_size cfg_block_size
            #n_head cfg_n_head
            #n_embd cfg_n_embd
          )
          #attn_pdrop cfg_attn_pdrop
          #resid_pdrop cfg_resid_pdrop
          #embd_pdrop cfg_embd_pdrop
          vs
      in
      let lin1 =
        ~(Layer.gen_linear {[sz_b, sz_t]}
            #input_dim cfg_n_embd
            #output_dim (4 * cfg_n_embd)
          )
          (let open VarStore in vs / "lin1")
          #use_bias true
          Layer.Activation.none
      in
      let lin2 =
        ~(Layer.gen_linear {[sz_b, sz_t]}
            #input_dim (4 * cfg_n_embd)
            #output_dim cfg_n_embd
          )
          (let open VarStore in vs / "lin2")
          #use_bias true
          Layer.Activation.none
      in
      ~Layer.gen_of_fn_ (fun #is_training (is_training : Bool) -> fun (xs : Tensor %[sz_b, sz_t, sz_c]) ->
          let xs =
            ~Tensor.gen_add
              xs
              (xs |> ~Layer.gen_forward ln1 |> ~Layer.gen_forward_ attn #is_training is_training)
          in
          let ys =
            xs
              |> ~Layer.gen_forward ln2
              |> ~Layer.gen_forward lin1
              |> ~Tensor.gen_gelu
              |> ~Layer.gen_forward lin2
              |> ~Tensor.gen_dropout #p cfg_resid_pdrop #is_training is_training
          in
          ~Tensor.gen_add xs ys
      )
  )
in

let gen_gpt
    {sz_b : Nat}
    {sz_t : Nat}
    {sz_c : Nat}
    #block_size (cfg_block_size : Int)
    #n_head (cfg_n_head : Int)
    #n_embd (cfg_n_embd : {n : Int | mod n cfg_n_head == 0})
    #vocab_size (cfg_vocab_size : Int) =
  &(fun #n_layer (cfg_n_layer : Int) ->
    fun #attn_pdrop (cfg_attn_pdrop : Float) ->
    fun #resid_pdrop (cfg_resid_pdrop : Float) ->
    fun #embd_pdrop (cfg_embd_pdrop : Float) ->
    fun(vs : VarStore) ->
      let tok_emb =
        ~(Layer.gen_embeddings {[sz_b, sz_t]} #num_embeddings cfg_vocab_size #embedding_dim cfg_n_embd)
          (let open VarStore in vs / "tok_emb")
      in
      let pos_emb =
        ~(VarStore.gen_new_var #shape [1, cfg_block_size, cfg_n_embd])
          vs
          #name "pos_emb"
          #init VarStore.Init.zeros
      in
      let ln_f =
        ~(Layer.gen_layer_norm #dim cfg_n_embd {[sz_b, sz_t, cfg_n_embd]})
          (let open VarStore in vs / "ln_f")
      in
      let head =
        ~(Layer.gen_linear {[sz_b, sz_t]} #input_dim cfg_n_embd #output_dim cfg_vocab_size)
          (let open VarStore in vs / "head")
          #use_bias false
          Layer.Activation.none
      in
      let blocks =
        let rec loop (i : Int) : #is_training Bool -> Tensor %[sz_b, sz_t, sz_c] -> Tensor %[sz_b, sz_t, sz_c] =
          if i <= 0 then
            fun #is_training (is_training : Bool) -> fun (xs : Tensor %[sz_b, sz_t, sz_c]) -> xs
          else
            fun #is_training (is_training : Bool) -> fun (xs : Tensor %[sz_b, sz_t, sz_c]) ->
              let xs =
                ~Layer.gen_forward_
                  ( ~(gen_block
                        {sz_b}
                        {sz_t}
                        {sz_c}
                        #block_size cfg_block_size
                        #n_head cfg_n_head
                        #n_embd cfg_n_embd
                    )
                      #attn_pdrop cfg_attn_pdrop
                      #resid_pdrop cfg_resid_pdrop
                      #embd_pdrop cfg_embd_pdrop
                      (let open VarStore in vs // i)
                  )
                  #is_training is_training
                  xs
              in
              loop (i - 1) #is_training is_training xs
        in
        ~Layer.gen_of_fn_ (loop cfg_n_layer)
      in
      ~Layer.gen_of_fn_ (fun #is_training (is_training : Bool) -> fun (xs : Tensor %[sz_b, sz_t]) ->
          let tok_emb = ~Layer.gen_forward tok_emb xs in
          let pos_emb = ~(Tensor.gen_narrow #dim 1 #length sz_t) #start 0 pos_emb in
          ~Tensor.gen_add tok_emb pos_emb (* [sz_b, sz_t, cfg_n_embd] *)
            |> ~Tensor.gen_dropout #p cfg_embd_pdrop #is_training is_training
            |> ~Layer.gen_forward_ blocks #is_training is_training (* [sz_b, sz_t, cfg_n_embd] *)
            |> ~Layer.gen_forward ln_f
            |> ~Layer.gen_forward head (* [sz_b, sz_t, cfg_vocab_size] *)
      )
  )
in

let gen_sample {cfg_vocab_size : Nat} (cfg_block_size : Nat) =
  &(let sampling_length = 2048 in
    let temperature = 1.0 in
    fun #gpt (gpt : #is_training Bool -> Tensor %[1, cfg_block_size] -> Tensor %[1, seq_len, cfg_vocab_size]) -> (* TODO *)
    fun #dataset (dataset : TextHelper %cfg_vocab_size) ->
    fun #device (device : Device) ->
      let input = ~(Tensor.gen_zeros [1, cfg_block_size]) (* #device device *) (* #kind (T Int64) *) in
      let rec loop (i : Int) (input : Tensor %[1, cfg_block_size]) : List Char =
        if i <= 0 then
          [] as List Char
        else
          ( Gc.full_major ();
            let logits =
              input
                |> ~Layer.gen_forward_ gpt #is_training false
                |> ~(Tensor.gen_select #dim 1) #index -1
            in
            let logits = ~Tensor.gen_div logits (Tensor.f temperature) in
            let sampled_y =
              ~(Tensor.gen_softmax #dim -1) logits (* #dtype (T Float) *)
                |> ~(Tensor.gen_multinomial #num_samples 1) #replacement true
            in
            let sampled_char =
              ~TextHelper.gen_char dataset #label (~Tensor.gen_int_value sampled_y)
            in
            let input =
              ~(Tensor.gen_cat_ #dim 1) input (~(Tensor.gen_view #size [1, 1]) sampled_y)
                |> ~(Tensor.gen_narrow #dim 1 #length cfg_block_size) #start 1
            in
            sampled_char :: loop (i - 1) input
          )
      in
      String.from_char_list (loop (sampling_length - 1) input)
  )
in

let gen_train
    {labels : Nat}
    #block_size (cfg_block_size : Int)
    #vocab_size (cfg_vocab_size : Int)
    #gpt (gpt : (n : Nat) -> &(#is_training Bool -> Tensor %[n, cfg_block_size] -> Tensor %[n, seq_len, cfg_vocab_size])) =
  &(let learning_rate = 0.0003 in
    let epochs = 100 in
    fun(vs : VarStore) ->
    fun #dataset (dataset : TextHelper %labels) ->
      let device = VarStore.device vs in
      let adam = Optimizer.adam vs #learning_rate learning_rate in
      let batches_per_epoch =
        (~TextHelper.gen_total_length dataset - ~(lift_int seq_len)) // ~(lift_int batch_size)
      in
      Checkpointing.loop
        #start_index 1
        #end_index epochs
        #var_stores [vs]
        #checkpoint_base "min-gpt.ot"
        #checkpoint_every_iters 1
        (fun #index (epoch_idx : Int) ->
          IO.write_all
            ("out.txt." ++ show_int epoch_idx)
            #data (~(gen_sample cfg_block_size) #gpt ~(gpt 1) #dataset dataset #device device);
          let start_time = Unix.gettimeofday () in
          let sum_loss = Tensor.f 0.0 in
          ~(TextHelper.gen_iter #seq_len seq_len #batch_size batch_size)
            ( fun(batch_idx : Int) ->
              fun #xs (xs : Tensor %[batch_size, seq_len]) ->
              fun #ys (ys : Tensor %[batch_size, seq_len]) ->
                let logits = ~Layer.gen_forward_ ~(gpt batch_size) #is_training true xs in
                (* Compute the cross-entropy loss. *)
                let loss =
                  ~Tensor.gen_cross_entropy_for_logits
                    (~(Tensor.gen_view #size [batch_size * seq_len, cfg_vocab_size]) logits)
                    #targets (~(Tensor.gen_view #size [batch_size * seq_len]) ys)
                in
                ~Tensor.gen_add_update sum_loss loss;
                print_string (show_int batch_idx ++ "/" ++ show_int batches_per_epoch);
                print_float (~Tensor.gen_float_value sum_loss / float (1 + batch_idx));
                Optimizer.backward_step #clip_grad (Optimizer.ClipGrad.norm2 4.0) adam #loss loss
            )
            #device device
            dataset;
          print_int epoch_idx;
          print_int (Unix.gettimeofday () - start_time);
          print_float (~Tensor.gen_float_value sum_loss / float batches_per_epoch))
  )
in

let labels = 1000 in (* TODO *)

&(let device = ~(Device.gen_cuda_if_available ()) in
  let dataset = ~(TextHelper.gen_create {labels} #filename "data/input.txt") in
  let vs = VarStore.create #frozen false #name "min-gpt" #device device () in
  print_string
    ( "Dataset loaded, length: "
        ++ (show_int (~TextHelper.gen_total_length dataset))
        ++ ", labels: "
        ++ (show_int ~(lift_int labels))
        ++ ".\n");
  ~(let gpt (n : Nat) =
      &(~(gen_gpt
            {n} (* sz_b *)
            {128} (* sz_t == seq_len ? *)
            {512} (* sz_c == cfg_n_embd ? *)
            #block_size seq_len
            #n_head 8
            #n_embd 512
            #vocab_size labels
        )
          #n_layer 8
          #attn_pdrop 0.1
          #resid_pdrop 0.1
          #embd_pdrop 0.1
          vs
      )
    in
    &(~(gen_train
          #block_size seq_len
          #vocab_size labels
          #gpt gpt
      )
        vs
        #dataset dataset
    )
  )
)
