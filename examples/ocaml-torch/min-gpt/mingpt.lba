(* This example uses the tinyshakespeare dataset which can be downloaded at:
   https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

   It has been heavily inspired by https://github.com/karpathy/minGPT
*)

let batch_size = 64 in
let seq_len = 128 in

(*
type config (vocab_size, n_embd, n_head, n_layer, block_size) =
  { vocab_size : { v:int | v = vocab_size }
  ; n_embd : { v:int | v = n_embd && n_embd = n_head * (n_embd // n_head) }
  ; n_head : { v:int | v = n_head }
  ; n_layer : { v:int | v = n_layer }
  ; block_size : { v:int | v = block_size }
  ; attn_pdrop : float
  ; resid_pdrop : float
  ; embd_pdrop : float
  }
*)

let gen_causal_self_attention
    {sz_b : Nat}
    {sz_t : Nat}
    {sz_c : Nat}
    (cfg_block_size : Int)
    (cfg_n_head : Int)
    (cfg_n_embd : {n : Int | mod n cfg_n_head == 0}) =
  &(fun(cfg_attn_pdrop : Float) ->
    fun(cfg_resid_pdrop : Float) ->
    fun(cfg_embd_pdrop : Float) ->
    fun(vs : VarStore) ->
      let linear (n : String) =
        ~(Layer.gen_linear {[4423]} #input_dim cfg_n_embd #output_dim cfg_n_embd) (* TODO *)
          (let open VarStore in vs / n)
          #use_bias true
          Layer.Activation.none
      in
      let key = linear "key" in
      let query = linear "query" in
      let value = linear "value" in
      let proj = linear "proj" in
      let mask_init =
        ~(Tensor.gen_ones [cfg_block_size, cfg_block_size]) (* #device (VarStore.device vs) *)
          |> ~Tensor.gen_tril #diagonal 0
          |> ~(Tensor.gen_view #size [1, 1, cfg_block_size, cfg_block_size])
      in
      let mask = ~Tensor.gen_eq_scalar mask_init (Tensor.f 0.0) in
      ~Layer.gen_of_fn_ (fun #is_training (is_training : Bool) -> fun (xs : Tensor %[sz_b, sz_t, sz_c]) ->
        ~(let sz_c_over_n_head = sz_c // cfg_n_head in
          let size = [sz_b, sz_t, cfg_n_head, sz_c_over_n_head] in
          &(let k = (* Tensor %[sz_b, cfg_n_head, sz_t, sz_c / cfg_n_head] *)
              xs
                |> ~Layer.gen_forward key
                |> ~(Tensor.gen_view #size size)
                |> ~(Tensor.gen_transpose #dim0 1 #dim1 2)
            in
            let q =
              xs
                |> ~Layer.gen_forward query
                |> ~(Tensor.gen_view #size size)
                |> ~(Tensor.gen_transpose #dim0 1 #dim1 2)
            in
            let v =
              xs
                |> ~Layer.gen_forward value
                |> ~(Tensor.gen_view #size size)
                |> ~(Tensor.gen_transpose #dim0 1 #dim1 2)
            in
            let att = (* Tensor %[sz_b, n_head, sz_t, sz_t] *)
              ~Tensor.gen_matmul q (~(Tensor.gen_transpose #dim0 -2 #dim1 -1) k)
            in
            let att =
              (~Tensor.gen_div att (Tensor.f (sqrt (float ~(lift_int sz_c_over_n_head)))))
                |> ~Tensor.gen_masked_fill #mask mask #value (Tensor.f neg_infinity)
                |> ~(Tensor.gen_softmax #dim -1)
                |> ~Tensor.gen_dropout #p cfg_attn_pdrop #is_training is_training
            in
            ~Tensor.gen_matmul att v (* [sz_b, n_head, sz_t, sz_c / n_head] *)
              |> ~(Tensor.gen_transpose #dim0 1 #dim1 2) (* [sz_b, sz_t, n_head, sz_c / n_head] *)
              |> ~Tensor.gen_contiguous
              |> ~(Tensor.gen_view #size [sz_b, sz_t, sz_c])
              |> ~Layer.gen_forward proj
              |> ~Tensor.gen_dropout #p cfg_resid_pdrop #is_training is_training
          )
        )
      )
  )
in

let gen_block
    {sz_b : Nat} {sz_t : Nat} {sz_c : Nat}
    (cfg_block_size : Int)
    (cfg_n_head : Int)
    (cfg_n_embd : {n : Int | mod n cfg_n_head == 0})
    {shape : {s : List Nat | List.length s >= 1 && List.last s == cfg_n_embd}} =
  &(fun(cfg_attn_pdrop : Float) ->
    fun(cfg_resid_pdrop : Float) ->
    fun(cfg_embd_pdrop : Float) ->
    fun(vs : VarStore) ->
      let ln1 = ~(Layer.gen_layer_norm #dim cfg_n_embd {shape}) (let open VarStore in vs / "ln1") in
      let ln2 = ~(Layer.gen_layer_norm #dim cfg_n_embd {shape}) (let open VarStore in vs / "ln2") in
      let attn =
        ~(gen_causal_self_attention
            {sz_b}
            {sz_t}
            {sz_c}
            cfg_block_size
            cfg_n_head
            cfg_n_embd
          )
          cfg_attn_pdrop
          cfg_resid_pdrop
          cfg_embd_pdrop
          vs
      in
      let lin1 =
        ~(Layer.gen_linear {shape}
            #input_dim cfg_n_embd
            #output_dim (4 * cfg_n_embd)
          )
          (let open VarStore in vs / "lin1")
          #use_bias true
          Layer.Activation.none
      in
      let lin2 =
        ~(Layer.gen_linear {List.append (List.init shape) [4 * cfg_n_embd]}
            #input_dim (4 * cfg_n_embd)
            #output_dim cfg_n_embd
          )
          (let open VarStore in vs / "lin2")
          #use_bias true
          Layer.Activation.none
      in
      ~Layer.gen_of_fn_ (fun #is_training (is_training : Bool) -> fun (xs : Tensor %shape) ->
          let xs =
            ~Tensor.gen_add
              xs
              (xs |> ~Layer.gen_forward ln1 |> ~Layer.gen_forward_ attn #is_training is_training)
          in
          let ys =
            xs
              |> ~Layer.gen_forward ln2
              |> ~Layer.gen_forward lin1
              |> ~Tensor.gen_gelu
              |> ~Layer.gen_forward lin2
              |> ~Tensor.gen_dropout #p cfg_resid_pdrop #is_training is_training
          in
          ~Tensor.gen_add xs ys
      )
  )
in

let fooo = [8901] in (* TODO *)

let gen_gpt
    {sz_b : Nat}
    {sz_t : Nat}
    {sz_c : Nat}
    (cfg_block_size : Int)
    (cfg_n_head : Int)
    (cfg_n_embd : {n : Int | mod n cfg_n_head == 0})
    (cfg_vocab_size : Int) =
  &(fun(cfg_n_layer : Int) ->
    fun(cfg_attn_pdrop : Float) ->
    fun(cfg_resid_pdrop : Float) ->
    fun(cfg_embd_pdrop : Float) ->
    fun(vs : VarStore) ->
      let tok_emb =
        ~(Layer.gen_embeddings {[sz_b, sz_t]} #num_embeddings cfg_vocab_size #embedding_dim cfg_n_embd)
          (let open VarStore in vs / "tok_emb")
      in
      let pos_emb =
        ~(VarStore.gen_new_var #shape [1, cfg_block_size, cfg_n_embd])
          vs
          #name "pos_emb"
          #init VarStore.Init.zeros
      in
      let ln_f =
        ~(Layer.gen_layer_norm #dim cfg_n_embd {[sz_b, sz_t, cfg_n_embd]})
          (let open VarStore in vs / "ln_f")
      in
      let head =
        ~(Layer.gen_linear {[sz_b, sz_t, cfg_vocab_size]} #input_dim cfg_n_embd #output_dim cfg_vocab_size)
          (let open VarStore in vs / "head")
          #use_bias false
          Layer.Activation.none
      in
      let blocks =
        let rec loop (i : Int) : #is_training Bool -> Tensor %fooo -> Tensor %fooo =
          if i <= 0 then
            fun #is_training (is_training : Bool) -> fun (xs : Tensor %fooo) -> xs
          else
            fun #is_training (is_training : Bool) -> fun (xs : Tensor %fooo) ->
              let xs =
                ~Layer.gen_forward_
                  ( ~(gen_block
                        {sz_b}
                        {sz_t}
                        {sz_c}
                        cfg_block_size
                        cfg_n_head
                        cfg_n_embd
                        {fooo}
                    )
                      cfg_attn_pdrop
                      cfg_resid_pdrop
                      cfg_embd_pdrop
                      (let open VarStore in vs // i)
                  )
                  #is_training is_training
                  xs
              in
              loop (i - 1) #is_training is_training xs
        in
        ~Layer.gen_of_fn_ (loop cfg_n_layer)
      in
      ~Layer.gen_of_fn_ (fun #is_training (is_training : Bool) -> fun (xs : Tensor %[sz_b, sz_t]) ->
          let tok_emb = ~Layer.gen_forward tok_emb xs in
          let pos_emb = ~(Tensor.gen_narrow #dim 1 #length sz_t) #start 0 pos_emb in
          ~Tensor.gen_add tok_emb pos_emb (* [sz_b, sz_t, n_embd] *)
            |> ~Tensor.gen_dropout #p cfg_embd_pdrop #is_training is_training
            |> ~Layer.gen_forward_ blocks #is_training is_training (* [sz_b, sz_t, n_embd] *)
            |> ~Layer.gen_forward ln_f
            |> ~Layer.gen_forward head (* [sz_b, sz_t, cfg_vocab_size] *)
      )
  )
in

let gen_sample {labels : Nat} (cfg_block_size : Nat) =
  &(let sampling_length = 2048 in
    let temperature = 1.0 in
    fun #gpt (gpt : #is_training Bool -> Tensor %[1, cfg_block_size] -> Tensor %[3456]) -> (* TODO *)
    fun #dataset (dataset : TextHelper %labels) ->
    fun #device (device : Device) ->
      let input = ~(Tensor.gen_zeros [1, cfg_block_size]) (* #device device *) (* #kind (T Int64) *) in
      let rec loop (i : Int) (input : Tensor %[1, cfg_block_size]) : List Char =
        if i <= 0 then
          [] as List Char
        else
          ( Gc.full_major ();
            let logits =
              input
                |> ~Layer.gen_forward_ gpt #is_training false
                |> ~(Tensor.gen_select #dim 1) #index -1
            in
            let logits = ~Tensor.gen_div logits (Tensor.f temperature) in
            let sampled_y =
              ~(Tensor.gen_softmax #dim -1) logits (* #dtype (T Float) *)
                |> ~(Tensor.gen_multinomial #num_samples 1) #replacement true
            in
            let sampled_char =
              ~TextHelper.gen_char dataset #label (Tensor.int_value sampled_y)
            in
            let input =
              ~(Tensor.gen_cat_ #dim 1) input (~(Tensor.gen_view #size [1, 1]) sampled_y)
                |> ~(Tensor.gen_narrow #dim 1 #length cfg_block_size) #start 1
            in
            sampled_char :: loop (i - 1) input
          )
      in
      String.from_char_list (loop (sampling_length - 1) input)
  )
in

let foooo = [1868] in (* TODO *)
let baaar = [1925] in (* TODO *)

let gen_train
    {labels : Nat}
    (cfg_block_size : Int)
    (cfg_n_head : Int)
    (cfg_n_embd : {n : Int | mod n cfg_n_head == 0})
    (cfg_vocab_size : Int) =
  &(let learning_rate = 0.0003 in
    let epochs = 100 in
    fun(vs : VarStore) ->
    fun(cfg_n_layer : Int) ->
    fun(cfg_attn_pdrop : Float) ->
    fun(cfg_resid_pdrop : Float) ->
    fun(cfg_embd_pdrop : Float) ->
    fun #gpt (gpt : #is_training Bool -> Tensor %[1, cfg_block_size] -> Tensor %[3456]) -> (* TODO *)
    fun #dataset (dataset : TextHelper %labels) ->
      let device = VarStore.device vs in
      let adam = Optimizer.adam vs #learning_rate learning_rate in
      let batches_per_epoch =
        (~TextHelper.gen_total_length dataset - ~(lift_int seq_len)) // ~(lift_int batch_size)
      in
      Checkpointing.loop
        #start_index 1
        #end_index epochs
        #var_stores [vs]
        #checkpoint_base "min-gpt.ot"
        #checkpoint_every_iters 1
        (fun #index (epoch_idx : Int) ->
          IO.write_all
            ("out.txt." ++ show_int epoch_idx)
            #data (~(gen_sample cfg_block_size) #gpt gpt #dataset dataset #device device);
          let start_time = Unix.gettimeofday () in
          let sum_loss = Tensor.f 0.0 in
          ~(TextHelper.gen_iter
              #seq_len seq_len
              #batch_size batch_size
              &(fun(batch_idx : Int) ->
                fun #xs (xs : Tensor %[batch_size, seq_len]) ->
                fun #ys (ys : Tensor %[batch_size, seq_len]) ->
                  let logits = ~Layer.gen_forward_ gpt #is_training true xs in
                  (* Compute the cross-entropy loss. *)
                  let loss =
                    ~Tensor.gen_cross_entropy_for_logits
                      (~(Tensor.gen_view #size [batch_size * seq_len, cfg_vocab_size]) logits)
                      #targets (~(Tensor.gen_view #size [batch_size * seq_len]) ys)
                  in
                  ~Tensor.gen_add_update sum_loss loss;
                  print_string (show_int batch_idx ++ "/" ++ show_int batches_per_epoch);
                  print_float (Tensor.float_value sum_loss / float (1 + batch_idx));
                  Optimizer.backward_step #clip_grad (Optimizer.ClipGrad.norm2 4.0) adam #loss loss
              )
            )
            #device device
            dataset;
          print_int epoch_idx;
          print_int (Unix.gettimeofday () - start_time);
          print_float (Tensor.float_value sum_loss / float batches_per_epoch))
  )
in

let labels = 1000 in (* TODO *)

&(let device = ~(Device.gen_cuda_if_available ()) in
  let dataset = ~(TextHelper.gen_create {labels} #filename "data/input.txt") in
  let vs = VarStore.create #frozen false #name "min-gpt" #device device () in
  print_string
    ( "Dataset loaded, length: "
        ++ (show_int (~TextHelper.gen_total_length dataset))
        ++ ", labels: "
        ++ (show_int ~(lift_int labels))
        ++ ".\n");
(*
  let cfg =
    { vocab_size = labels
    ; n_embd = 512
    ; n_head = 8
    ; n_layer = 8
    ; block_size = seq_len
    ; attn_pdrop = 0.1
    ; resid_pdrop = 0.1
    ; embd_pdrop = 0.1
    }
  in
*)
  let gpt = gpt vs cfg in
  train vs #gpt gpt #cfg cfg #dataset dataset
)
(*
  case Sys.argv of
  | [_bin] | [_bin, "train"] ->
      train vs #gpt #cfg #dataset
  | [_bin, "sample", filename] ->
      let named_tensors = VarStore.all_vars vs in
      Serialize.load_multi_ #named_tensors #filename;
      sample cfg #gpt #dataset #device |> Stdio.print_endline
  | _ ->
      failwith "usage: mingpt (train|sample weight.ot)"
  end
*)
